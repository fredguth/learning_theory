\documentclass[10pt, professionalfonts]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage[brazil]{babel} % Use brazilian portuguese by default
\usepackage{amsmath}
\usepackage{graphicx, wrapfig}
\graphicspath{ {./imgs/} }
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}

\definecolor{myBeige}{HTML}{faf7f0}
\definecolor{myRed}{HTML}{e45d53}
\setbeamercolor{frametitle}{bg=myBeige, fg=normal text.fg}
\setbeamercolor{background canvas}{bg=myBeige, fg=normal text.fg}

% Configure title graphic positioning
\setbeamertemplate{title graphic}{%
  \vbox to 0pt {
    \vspace*{.75\textheight}  % change the value as necessary
    \hfill\inserttitlegraphic%
  }%
  \nointerlineskip%
}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{xspace}
















\title{Teoria de Aprendizagem Computacional}
\subtitle{Uma Brevíssima Introdução}
\date{\today}
\author{Fred Guth}
\institute{%
Departamento de Ciência da Computação\\%
Universidade de Brasília\\%
\par%
316415\\%
Seminários}

\graphicspath{{./imgs/}}

\titlegraphic{\includegraphics[width=2.5cm]{logoUnB.png}}


\begin{document}

\maketitle





{
\AtBeginSection{}
\section{Preâmbulo}
\begin{frame}{O que é Apreensível?}

\begin{columns}[c]
  \column{.6\textwidth}
\textbf{O problema da indução:}\\
Não há justificativa para generalizações do tipo~\cite{Hume2009Tratado}: 
\begin{quotation}
  ``Sempre que no passado comi pão, ele me alimentou. Portanto, da próxima vez que comer pão, ele me alimentará''.
\end{quotation}~\cite{hume2004investigacoes}
  \column{.3\textwidth}
  \centering 
  \begin{figure}
    \label{hume}
    \framebox{\includegraphics[width=1.5in]{hume}}
    \caption{David Hume (1711-1776).}
  \end{figure} 
\end{columns}
\end{frame}

  \begin{frame}{Teoria da Aprendizagem Computacional}

      \centering
      \centerline{\includegraphics<1>[width=1\textwidth]{venn_lt}}
      \includegraphics<2>[width=.4\textwidth]{venn_lt}

      \pause
      \only<2>{
        \begin{columns}[c]
          \column{.3\textwidth}
            \begin{figure}
              \label{valiant}
              \framebox{\includegraphics[width=1.3in]{leslie}}
              \caption{Leslie Valiant, Prêmio Turing 2010.}
            \end{figure}
          \column{.7\textwidth}
          \begin{quotation}
            ``Quantas amostras são necessárias para aprendermos um conceito?''
          \end{quotation}
          \hspace*{1cm}\url{learningtheory.org/colt19}~\cite{COLT}
        \end{columns}
      }
      
  \end{frame}
  \begin{frame}{Modelo de Aprendizado PAC: Definição Informal}
    \begin{equation}
      \underbrace{\text{ Provavelmente }}_{\textrm{confiança} >(1-\delta)}
      \underbrace{\text{ Aproximadamente }}_{\textrm{tolerância} \leq\epsilon}
      \underbrace{\text{   Correto   }}_{\textrm{Erro Absoluto} = 0} \nonumber
    \end{equation}
    
  \end{frame}
}




{
\AtBeginSection{}
\section{Modelo de Aprendizado PAC: Definição formal}
\begin{frame}{Visão Geral}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}
\begin{frame}{Modelo de Aprendizado PAC: Notações e Definições}
  \begin{description}\setlength\itemsep{1em}
    \item [$\mathcal{X}$]: espaço das instâncias;
    \item [$\mathcal{D}$]: distribuição de probabilidade em $\mathcal{X}$;
    \item [$\mathcal{Y}=\{0,1\}$]: rótulos;
    \item [$c:\mathcal{X}\to\mathcal{Y}$]: conceito $c \in \mathcal{C}$;
    \item [$\mathcal{A}$]: algoritmo aprendiz;
    \item [$S=\{(x_i, c(x_i))\}$]: conjunto de amostras de treinamento;
    \item [$h \in \mathcal{H}$]: hipótese do espaço de Hipóteses;
    \item [$erro_{S}(h)$]: erro de treinamento de $h$, $erro_{S}(h) \equiv \underset{x \sim \mathcal{S}}{P}[c(x) \neq h(x)]$;
    \item [$erro_{\mathcal{D}}(h)$]: erro absoluto de $h$, $erro_{\mathcal{D}}(h) \equiv \underset{x \sim \mathcal{D}}{Pr}[c(x) \neq h(x)]$;
    
  \end{description}
\end{frame}

\begin{frame}{Definições: conceito}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{concept}
    \label{concept}
    \caption{Um conceito $c$.}
  \end{figure}
\end{frame}
\begin{frame}{Definições: hipótese}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{hypothesis}
    \label{hypothesis}
    \caption{Uma hipótese $h \in \mathcal{H}$.}
  \end{figure}
\end{frame}
\begin{frame}{Definições: aprendiz}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{diagram}
    \label{diagram}
    \caption{Um aprendiz genérico $L$.}
  \end{figure}

\end{frame}
\begin{frame}{Definições: espaço de hipóteses  $\mathcal{H}$}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.95\textwidth]{hspaces}
    \label{hspaces}
    \caption{hipóteses pertencentes a diferentes espaços de hipóteses $\mathcal{H}$.}
\end{figure}

\end{frame}

\begin{frame}{Definições: erro de $h$}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{conceptVShypothesis}
    \label{conceptVShypothesis}
\end{figure}
Erro de Treinamento:\begin{equation}
  erro_{S}(h) \equiv \underset{x \sim \mathcal{S}}{Pr}[c(x) \neq h(x)]
\end{equation}
Erro de Absoluto:
\begin{equation}
  erro_{\mathcal{D}}(h) \equiv \underset{x \sim \mathcal{D}}{Pr}[c(x) \neq h(x)]
\end{equation}
\end{frame}




}
\begin{frame}{Modelo de Aprendizado PAC: Definição Formal}
  \noindent\fbox{%
    \parbox{\textwidth}{%
    
        $C$ é \textbf{PAC-apreensível} por $\mathcal{A}$  se e somente se, com probabilidade $1 - \delta$, $\mathcal{A}$  gera uma hipótese $h \in \mathcal{H}$ com $erro_{\mathcal{D}}(h)\leq\epsilon$ com número de amostras polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $n=$ tamanho da instância e $size(c)=$ tamanho do conceito, para $0 < \epsilon \leq \frac{1}{2}$ e $0 < \delta \leq \frac{1}{2}$.
    }%
  }

    \bigskip
    \noindent\fbox{%
    \parbox{\textwidth}{%
        $C$ é \textbf{eficientemente PAC-apreensível} por $\mathcal{A}$ se e somente se $\mathcal{A}$ é \textbf{PAC-apreensível} e gera hipótese em tempo polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $n$ e $tamanho(c)$~\cite{MitchelPAC}.
    }%
  }
  
  
\end{frame}
{

  \AtBeginSection{}
  \section{Teorema de Haussler}
  \begin{frame}{Teorema de Haussler (1988): Limite do erro absoluto}
    \noindent\fbox{%
      \parbox{\textwidth}{%
      \textbf{Teorema: } Seja $\mathcal{H}$ finito, $C \subset \mathcal{H}$ e $L$ um aprendiz consistente. Se $|S|=m, m \geq 1$, então 
        $Pr[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}$\cite{Haussler1988}
      }%
    }
    \begin{proof}  Sejam $h_i (i = 1, ..., k) \in \mathcal{H}$ tais que $erro_{\mathcal{D}}(h_i)>\epsilon$
      \begin{align}
      Pr_{x_j\sim\mathcal{S}}[(c(x_j) \neq h_i(x_j))=0] \leq (1-\epsilon) \label{haussler:a} \\
      Pr[\forall h \in |\mathcal{H}|: (erro_{S}(h)=0 \land erro_{\mathcal{D}}(h)> \epsilon)] \leq (1-\epsilon)^m \label{haussler:b} \\
      Pr[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq k (1-\epsilon)^m \label{haussler:c} \\
      Pr[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq |\mathcal{H}| (1-\epsilon)^m \label{haussler:d} \\ 
      (1-x) \leq e^{-x}, 0 \leq x \leq 1 \implies \nonumber \\ Pr[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}  \label{haussler:e} \\
      \qedhere
      \end{align}
      \end{proof}
      

  \end{frame}
\begin{frame}{Teorema de Haussler (1988): Implicações}
  \noindent\fbox{%
    \parbox{\textwidth}{%
    $|H|e^{-\epsilon m}=\delta \implies $ aprendiz $L$ aprende $C$ em $m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|}+\ln{\frac{1}{\delta}})$ amostras de treinamento. 
    }%
}

\begin{proof}  Dado $Pr[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}$ (Haussler) e $Pr[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq \delta$ (PAC), podemos supor:
\begin{align}
    |\mathcal{H}|e^{-\epsilon m}=\delta \implies e^{-\epsilon m} = \frac{\delta}{|\mathcal{H}|} \\
    - \epsilon m = (\ln{\delta} - \ln{|\mathcal{H}|})  \\
    \epsilon m = (\ln{|\mathcal{H}|} - \ln{\delta}) \\
    m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}}) \\
    \qedhere
\end{align}
\end{proof}

\end{frame}
}


{

  \AtBeginSection{}
\section{Conclusão}
\begin{frame}{Conclusão}
  \begin{itemize}[<+->]
    \item Aprendizagem Computacional = Teoria da Computação $\cap$  Aprendizagem de Máquina
    \item Podemos limitar \textbf{erro absoluto} mesmo só conhecendo o \textbf{erro de treinamento}
    \item Para determinados $\delta$ e $\epsilon$, o tamanho do conjunto de treinamento necessário \textbf{independe} de $\mathcal{C}$ ou  de $P(\mathcal{X})$.
    \item ... e depende apenas logaritmicamente de $|\mathcal{H}|$
    \item Na prática esses limites são muito "frouxos" \cite{MitchelPAC}: preferência por modelos pequenos
    \end{itemize}
\end{frame}
}
{

  \AtBeginSection{}
\section{Referências}
\begin{frame}[allowframebreaks]{Referências}

  \bibliography{references}
  \bibliographystyle{abbrv}

\end{frame}
}
\begin{frame}<beamer>[standout]
  Obrigado.
  \vspace*{.5cm}
\begin{center}\href{https://github.com/fredguth/learning_theory.git}{\url{github.com/fredguth/learning_theory}}\end{center}
  \vspace*{.5cm}
  \small{
    \begin{center}\ccbysa\end{center}
    \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons \\
      Attribution-ShareAlike 4.0 International License}.
  }
\end{frame}

\appendix

\begin{frame}[fragile]{Para saber mais}
  \begin{itemize}
    \item \href{http://bit.ly/tom_mitchel_learning_theory}{\url{http://bit.ly/tom_mitchel_learning_theory}}
    \item \href{https://www.youtube.com/watch?v=dYMCwxgl3vk&t=1594s}{Hypothesis Space and Inductive Bias: \url{https://www.youtube.com/watch?v=dYMCwxgl3vk&t=1594s}}
  \end{itemize}
\end{frame}



\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seminário
% LaTeX Template
% Version 1.0 (13/03/19)
%
% Author: Fred Guth (fredguth@fredguth.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Default font size is 10pt, can alternatively be 11pt or 12pt
a4paper, % Alternatively letterpaper for US letter
onecolumn, % Alternatively twocolumn
% portrait % Alternatively landscape
]{article}

\input{structure.tex} % Input the file specifying the document layout and structure
%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\papertitle{Uma Brevíssima Introdução à \\  Teoria da Aprendizagem Computacional} % The title of the article
\lectureabstract{Teoria de Aprendizagem Computacional é o ramo de estudo que objetiva aplicar Teoria da Complexidade para problemas de aprendizado. Nesta breve introdução, iremos apresentar, de forma intuitiva, o Modelo de Aprendizado-PAC (\emph{Probably Approximately Correct}) de Leslie Valiant (1984) que é o trabalho seminal da área. }
\datenotesstarted{12 de março de 2019} % The date when these notes were first made
\docdate{\datenotesstarted; rev. \today} % The date when the notes were lasted updated (automatically the current date)

\docauthor{Fred Guth} % Your name
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}
%----------------------------------------------------------------------------------------

\begin{document}

\pagestyle{myheadings} % Use custom headers
\markright{\papertitle} % Place the article information into the header

%----------------------------------------------------------------------------------------
%	PRINT ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\thispagestyle{plain} % Plain formatting on the first page

\printtitle % Print the title

%----------------------------------------------------------------------------------------
%	ARTICLE NOTES
%----------------------------------------------------------------------------------------

\section{O que é Apreensível?}
\begin{wrapfigure}{R}{1.5in}
    \framebox{\includegraphics[width=1.5in]{hume_d2}}
    \caption{David Hume (1711-1776).}
    \label{hume}
\end{wrapfigure}

O que é apreensível? Essa é uma questão anterior à Ciência da Computação.  No século 18, o filósofo escocês David Hume se perguntou se é possível gerar conhecimento a partir da indução (\textbf{O problema da Indução})\cite{Hume2009Tratado}.  O que é justamente a base do aprendizado supervisionado, a área da Inteligência Artificial mais pesquisada no momento.

Para Hume, não há justificativa lógica para generalizações baseadas em indução. 
\begin{quotation}
    "O pão que comi anteriormente alimentou-me, (...) mas segue-se porventura disso outro pão deva igualmente alimentar-me em outra ocasião?  Essa ocasião não parece de nenhum modo necessária"~\cite{hume2004investigacoes}.
\end{quotation}
Para ele nós vemos causalidade onde há apenas a conjunção constante entre dois acontecimentos distintos. Mas se isso é verdade, é possível obter conhecimento através da indução?  E se não é possível, o aprendizado indutivo supervisionado não tem justificativa? Pior, o método científico não pode ser logicamente justificado?

A abordagem de Valiant traz uma resposta ao mesmo tempo prática e formal para esse problema.  Não é possível dizer que "esse pão que ainda não comi vai me alimentar", sem comê-lo. Entretanto, é possível dizer, com certo nível de confiança que a hipótese "vai me alimentar" está aproximadamente correta. Em outras palavras, embora seja possível que o pão não me alimente (vai que está estragado?), na maioria das vezes ele me alimenta e é possível mensurar a probabilidade desta hipótese estar aproximadamente correta. 

Assim como a Teoria da Computação apresenta o conceito de computabilidade, Teoria da Aprendizagem apresenta o conceito de apreensibilidade, o que é e o que não é apreensível. 

% Confuso? Vamos colocar mais matemática nessa discussão para deixar tudo mais claro. Só que antes, algumas definições.

%------------------------------------------------
\section{Teoria da Aprendizagem Computacional} 

\begin{wrapfigure}{R}{1.5in}
    \framebox{\includegraphics[width=1.5in]{leslie_bg2}}
    \caption{Leslie Valiant, Prêmio Turing 2010.}
    \label{leslie}
\end{wrapfigure}

Teoria da Complexidade Computacional, um dos alicerces da Ciência da Computação, tem por objetivo examinar e classificar problemas de acordo com suas dificuldades de solução. Ao estudar complexidade de algoritmos, entusiastas de aprendizado de máquina podem naturalmente se perguntar: \begin{quotation}Será que não seria possível analisar qual seria a quantidade de amostras necessárias para um algoritmo aprender uma tarefa?\end{quotation}
    
A resposta é, naturalmente, sim. Em 1984, Leslie Valiant publicou o modelo de aprendizado \textbf{Provavelmente Aproximadamente Correto} (PAC, \emph{Probably Approximately Correct})\cite{Valiant1984}, justamente com o objetivo de incitar pesquisadores que estudavam complexidade de algoritmos a pensar problemas de aprendizado. 

Ele introduziu a ideia de problemas de aprendizado que são apreensíveis em tempo polinomial, \textbf{PAC apreensíveis}, em analogia com a classe dos problemas $P$. Pode-se dizer que foi bem sucedido:  diversos pesquisadores estenderam ou propuseram novas teorias, o que originou o ramo de estudo chamado de \textbf{Teoria de Aprendizado Computacional}, que tem até a sua própria conferência anual, COLT (\emph{Conference on Learning Theory})~\cite{COLT}.

Nesse trabalho, o objetivo é apresentar de forma intuitiva, reduzindo bastante os jargões, o Modelo de Aprendizado-PAC de Leslie Valiant que é o trabalho seminal da área. 



% %------------------------------------------------
\section{Modelo de Aprendizado-PAC}

\subsection{Definição informal}
\begin{equation}
    \underbrace{\text{ Provavelmente }}_{\textrm{confiança} >(1-\delta)}
    \underbrace{\text{ Aproximadamente }}_{\textrm{tolerância} \leq\epsilon}
    \underbrace{\text{   Correto   }}_{\textrm{Erro Absoluto} = 0} \nonumber
  \end{equation}
  No \textbf{modelo PAC}, uma tarefa é apreensível se existe um algoritmo capaz de aprendê-la de forma confiável em um número razoável (polinomial) de passos e amostras de treinamento.

\subsection{Definições e Notações}

Genericamente, podemos pensar em um algoritmo como uma função que transforma uma  instância do espaço de entrada, $\mathcal{X}$, em uma instância do espaço de soluções da tarefa que queremos realizar, $\mathcal{Y}$ (figura \ref{fig:function}). Em geral, essa função é um conjunto de regras programadas.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{function}
    \caption{Um algoritmo genérico: $\mathcal{X} \to \mathcal{Y}$.}
    \label{fig:function}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{concept}
    \caption{Um conceito $c$.}
    \label{fig:concept}
\end{figure}

No contexto de aprendizado de máquina, não sabemos expressar essa função alvo, portanto teremos que inferi-la, aprendê-la. Chamaremos essa função ideal de \textbf{conceito} (figura \ref{fig:concept}). É o conceito que queremos aprender. No modelo PAC, um conceito é uma função $c: \mathcal{X} \to \{+,-\}$, ou seja, uma função que mapeia o espaço das instâncias em um valor \emph{booleano}.


\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{hypothesis}
    \caption{Uma hipótese $h \in \mathcal{H}$.}
    \label{fig:hypothesis}
\end{figure}


A função realmente obtida, a regra, a heurística, chamamos de \textbf{hipótese}(figura \ref{fig:hypothesis}). 

\begin{figure}[!htp]
    \centering
    \begin{tikzpicture}
        \node[draw, rectangle, minimum width = 3 cm, minimum height = 2 cm] (fl) at (0,0) {$\mathcal{H}$};
        \node[above] at (fl.north) {$\mathcal{A}$};
        \draw[<-] (fl) -- node[above]{$S = \{x_i, c(x_i)\}$} node[below]{} ++(-5,0);
        \draw[->] (fl) -- node[above]{$h$} node[below]{} ++(5,0);
    \end{tikzpicture}
    \caption{Um aprendiz genérico $\mathcal{A}$.}
    \label{fig:diagram}
  \end{figure}


  \begin{figure}[!htp]
      \centering
      \includegraphics[width=.95\textwidth]{hspaces}
      \caption{hipóteses pertencentes a diferentes espaços de hipóteses $\mathcal{H}$.}
      \label{fig:hspaces}
  \end{figure}
O aprendiz (figura \ref{fig:diagram}), portanto, escolhe uma hipótese $h$
restrita às generalizações permitidas e preferidas pelo espaço de hipóteses $\mathcal{H}$ (figura \ref{fig:hspaces}).
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{conceptVShypothesis}
    \caption{O erro, $c(x)\neq h(x)$} .
    \label{fig:erros}
\end{figure}
O objetivo do aprendiz $\mathcal{A}$ é generalizar bem, ou seja,  escolher uma hipótese com menor erro na distribuição desconhecida $\mathcal{D}=P(\mathcal{X})$, menor \textbf{erro absoluto} ($erro_{\mathcal{D}}$). Entretanto, $\mathcal{A}$ só tem conhecimento do \textbf{erro de treinamento}, $erro_{S}(h)$ (figura \ref{fig:erros}). Podemos formular o erro absoluto e o erro de treinamento da seguinte forma~\cite{MitchelPAC}:
\begin{equation}
    erro_{\mathcal{D}}(h) \equiv P_{x \sim \mathcal{D}}[c(x) \neq h(x)]
\end{equation}
\begin{equation}
  erro_{S}(h) \equiv P_{x \sim S}[c(x) \neq h(x)]
\end{equation}

\subsection{Modelo PAC: Definição formal}

\noindent\fbox{%
    \parbox{\textwidth}{%
        $\mathcal{C}$ é \textbf{PAC-apreensível} por $\mathcal{A}$ se e somente se, com probabilidade $1 - \delta$, $\mathcal{A}$ gera uma hipótese $h \in \mathcal{H}$ com $erro_{\mathcal{D}}(h)\leq\epsilon$ com número de amostras polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $n = |x|$ e $|c|$, para $0 < \epsilon \leq \frac{1}{2}$ e $0 < \delta \leq \frac{1}{2}$.
    }%
  }
  
  \bigskip

  \noindent\fbox{%
  \parbox{\textwidth}{%
      $\mathcal{C}$ é \textbf{eficientemente PAC-apreensível} por $\mathcal{A}$ se e somente se $\mathcal{A}$ é \textbf{PAC-apreensível} e gera hipótese em tempo polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $|x|$ e $|c|$.
  }%
}

\bigskip

\subsection{Teorema de Haussler (1988): Limites para $\mathcal{H}$ finito.}


Para espaços de hipóteses finito e que contém o conceito que se quer aprender, o teorema de Haussler\cite{Haussler1988} dá um limite inferior ao erro absoluto esperado dado um número de amostras $m$ ou o limite mínimo de amostras necessárias $m$ para um aprendiz consistente aprender a classe de conceitos $\mathcal{C}$ com níveis aceitáveis de precisão e confiança.

\bigskip

\noindent\fbox{%
    \parbox{\textwidth}{%
    \textbf{Teorema: } Seja $\mathcal{H}$ finito. Seja $\mathcal{A}$ um aprendiz que para qualquer conceito alvo $c$ e distribuição desconhecida $\mathcal{D}$ retorna uma hipótese consistente $h: erro_{S}(h)=0$. Seja $|S|=m, m \geq 1$, então \\
      $P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}$
    }%
  }

  \begin{proof}  Sejam $h_i (i = 1, ..., k)$ todas as hipóteses em $\mathcal{H}$ tais que $erro_{\mathcal{D}}(h_i)>\epsilon$, temos:
    \begin{align}
        P_{x_j\sim\mathcal{S}}[(c(x_j) \neq h_i(x_j))=0] \leq (1-\epsilon) \label{haussler:a} \\
        P[\forall h \in |\mathcal{H}|: (erro_{S}(h)=0 \land erro_{\mathcal{D}}(h)> \epsilon)] \leq (1-\epsilon)^m \label{haussler:b} \\
        P[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq k (1-\epsilon)^m \label{haussler:c} \\
        P[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq |\mathcal{H}| (1-\epsilon)^m \label{haussler:d} \\ 
        (1-x) \leq e^{-x}, 0 \leq x \leq 1 \implies P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}  \label{haussler:e} \\
        \qedhere
    \end{align}
    \end{proof}

A probabilidade de uma hipótese "ruim" ($erro_{\mathcal{D}}(h_i)>\epsilon$) prever corretamente uma determinada amostra é $\leq (1-\epsilon)$ (eq. \ref{haussler:a}). Portanto, a probabilidade de uma hipótese "ruim" prever corretamente todas as amostras de treinamento é  $\leq (1-\epsilon)^m$ (eq. \ref{haussler:b}). Há hipóteses "boas" e "ruins", imaginando que há $k$ hipóteses "ruins", a probabilidade de alguma destas $k$ hipóteses "ruins" prever corretamente todo o conjunto de treinamento é $\leq k (1-\epsilon)^m$ (eq. \ref{haussler:c}). Como essas hipóteses "ruins" pertencem a $\mathcal{H}$, $k$ é necessariamente menor que $|\mathcal{H}|$, portanto chegamos ao limite do erro absoluto de $h$ dado uma precisão $\epsilon$  e um número de amostras $m$ (eq. \ref{haussler:d}).

Como dito anteriormente, uma outra forma de ver o Teorema de Haussler é que para uma determinada tolerância ao erro $\epsilon$ e confiança $(1-\delta)$, o teorema dá o limite inferior no número de amostras de treinamento, $m \geq \frac{1}{\epsilon}(\ln{|\mathcal{H}|}+\ln{\frac{1}{\delta}})$,  para se aprender a classe de conceitos $\mathcal{C}$.

\bigskip

\noindent\fbox{%
    \parbox{\textwidth}{%
    $|H|e^{-\epsilon m}=\delta \implies $ aprendiz $\mathcal{A}$ aprende $\mathcal{C}$ em $m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|}+\ln{\frac{1}{\delta}})$ amostras de treinamento. 
    }%
}

\begin{proof}  Dado que o  Teorema de Haussler demonstra que $|\mathcal{H}|e^{-\epsilon m}$ é um limite inferior para a probabilidade do \textbf{erro absoluto} ultrapassar a tolerância ao erro ($\epsilon$) e que o modelo de aprendizado-PAC tem como limite inferior desta mesma probabilidade o valor $\delta$, podemos supor $|\mathcal{H}|e^{-\epsilon m}=\delta$, consequentemente:
\begin{align}
    |\mathcal{H}|e^{-\epsilon m}=\delta \implies e^{-\epsilon m} = \frac{\delta}{|\mathcal{H}|} \\
    - \epsilon m = (\ln{\delta} - \ln{|\mathcal{H}|})  \\
    \epsilon m = (\ln{|\mathcal{H}|} - \ln{\delta}) \\
    m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}}) \label{sample_complexity}\\
    \qedhere
\end{align}
\end{proof}


Interessante notar que esse limite inferior no número de amostras é idependente de $\mathcal{C}$ ou $\mathcal{D}$ e logarítmico em relação ao tamanho do espaço $\mathcal{H}$~\cite{Haussler1988}.
% %------------------------------------------------

\section{Exemplos de aplicação do modelo de aprendizado-PAC}
\subsection{A classe universal de conceitos é PAC-apreensível?}
Seja o universo das instâncias $\mathcal{X}=\{0,1\}^{n}$, o espaço de vetores booleanos de tamanho n. A classe de conceitos $\mathcal{U}_n$ é formada por todos os subconjuntos de $\mathcal{X}$, portanto, pode ser considerada como uma classe universal de conceitos já que contém todas as possíveis classificações para o espaço das instâncias dado.
\begin{align}
    |\mathcal{U}_n| = 2^{|\mathcal{X}|}&=2^{(2^m)} \\
    |\mathcal{H}| &\geq |\mathcal{U}_n| \\
    |\mathcal{H}| &\geq 2^{(2^m)}
\end{align}
Com a eq. \ref{sample_complexity}, temos:
\begin{align}
    m \geq \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}})\\
    m \geq \frac{1}{\epsilon}((2^m)\ln{2} + \ln{\frac{1}{\delta}}) \\
    \therefore m \in O(2^m, \frac{1}{\epsilon}, ln{\frac{1}{\delta}})
\end{align}
Logo, \textbf{$\mathcal{U}_n$ não é PAC-apreensível}. \begin{quote} There is no free-lunch.\end{quote}
\subsection{A classe de conjunções de n literais é PAC-apreensível?}
Seja o universo das instâncias $\mathcal{X}=\{x_1, \ldots, x_n\}$, $n$ literais, ou seja, $x_i \lor \bar{x_i}=1$ . 

A classe de conceitos representáveis por uma conjunção de até n literais, $C_n$ é PAC-apreensível?

\begin{align}
    |C_n| = 3^n 
\end{align}
uma vez que cada literal pode ser verdadeiro, falso ou não estar em $C_n$.
Com a eq. \ref{sample_complexity}, temos:
\begin{align}
    m \geq \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}})\\
    m \geq \frac{1}{\epsilon}(n\ln{3} + \ln{\frac{1}{\delta}}) \\
    \therefore m \in O(n, \frac{1}{\epsilon}, ln{\frac{1}{\delta}})
\end{align}
Logo, \textbf{$\mathcal{U}_n$ é PAC-apreensível}. Além disso, é fácil provar que $C_n$ \textbf{eficientemente PAC-apreensível}, já que o seguinte algoritmo polinomial em função do número de literais pode ser usado: \\ \\
Seja $ B_j = {b_1, \ldots, b_n}$ uma amostra para aprender uma expressão $c \in C_n$. \\ \\
Para todos os $3^n$ possíveis valores de $B_j$:
\begin{description}
    \item  $(B_j=1 \land b_i=0) \implies x_i \notin C_n$;
    \item  $(B_j=1 \land b_i=1) \implies \bar{x_i} \notin C_n$;
    \item  $(B_j=0) \implies $ não há nada a aprender com esta amostra.
    \item  Quando todos os exemplos positivos de $B_j$ tiverem sido processados, retorne a conjunção de $x_i \in C_n$. 
\end{description}
Como $C_n$ é PAC-apreensível e há um algoritmo polinomial em n e  $|C_n|$ para obter $h$, $C_n$ \textbf{é eficientemente PAC-apreensível}.


\subsection{A classe de fórmulas k-CNF é PAC-apreensível?}
Uma fórmula k-CNF é uma conjunção em que cada termo tem até $k$ literais, $\bigwedge_{1}^{n}(\bigvee_{1}^{k})$. Portanto, seja $\mathcal{C}_{k-CNF}$ a classe de todos os conceitos que podem ser representados por uma fórmula k-CNF, $\mathcal{C}_{k-CNF}$ é PAC-apreensível?

\begin{align}
    |\mathcal{C}_{k-CNF}| = (3^k)^n = 3 ^{k \dot n} \label{kcnf}
\end{align}

Com a eq. \ref{sample_complexity}, temos:
\begin{align}
    m \geq \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}})\\
    m \geq \frac{1}{\epsilon}(\cancel{k} n \ln{3} + \ln{\frac{1}{\delta}}) \\
    \therefore m \in O(n, \frac{1}{\epsilon}, ln{\frac{1}{\delta}}) 
\end{align}
Logo, \textbf{$\mathcal{C}_{k-CNF}$ é PAC-apreensível}. 

Como $C_{k-CNF}$ é reduzível a $C_n$ \begin{equation}
C_{k-DNF}\leq_{m}C_n \end{equation}~\cite[pg. 18]{FoundationsML}
, \textbf{$\mathcal{C}_{k-CNF}$ é eficientemente PAC-apreensível}. 


\subsection{A classe de fórmulas k-DNF é PAC-apreensível?}
Uma fórmula k-DNF é uma conjunção em que cada termo tem até $k$ literais, $\bigvee_{1}^{n}(\bigwedge_{1}^{k})$. Portanto, seja $\mathcal{C}_{k-CNF}$ a classe de todos os conceitos que podem ser representados por uma fórmula k-CNF, $\mathcal{C}_{k-CNF}$ é PAC-apreensível?

É fácil ver que $|\mathcal{C}_{k-DNF}| = 3 ^{k \dot n}$ pelo mesmo raciocínio da eq. \ref{kcnf}. 
\begin{align}
    |\mathcal{C}_{k-DNF}| = (3^k)^n = 3 ^{k \dot n}
\end{align}
Logo, pelos mesmos motivos de $C_{k-DNF}$, \textbf{$\mathcal{C}_{k-CNF}$ é PAC-apreensível}. 

Entretanto, \textbf{$\mathcal{C}_{k-DNF}$ não é eficientemente PAC-apreensível}, uma vez que é possível reduzir o problema 3-coloring de grafos para o 3-DNF:
\begin{align}
\textrm{3-coloring} \leq C_{k-DNF} \\
\therefore C_{k-DNF} \in NP 
\end{align}
~\cite[pg. 17-18]{FoundationsML}
% %------------------------------------------------

\section{Conclusão}

Teoria de Aprendizagem Computacional traz um arcabouço teórico que nos permite estimar o número de amostras de treinamento necessárias para um algoritmo convergir, com alta probabilidade, para uma boa generalização. 

Em particular no caso de $\mathcal{H}$ finito, para convergir com determinado nível de confiança e tolerância a erro, o limite inferior no tamanho do conjunto de treinamento é independente da classe de conceitos alvo ou da distribuição das instâncias de entrada e depende apenas logaritmicamente do tamanho do espaço de hipóteses. 

Na prática, no entanto, esses limites são muito "frouxos"\cite{MitchelPAC} e dão a falsa impressão que modelos mais simples (menor espaço de hipóteses) são sempre melhores, o que não corresponde com a realidade em que modelos complexos e grandes como redes neurais profundas apresentam melhores generalizações para um grande número de aplicações.

Um outro ponto interessante apresentado é o fato da classe de conceitos k-CNF ser eficientemente PAC-apreensível, enquanto k-DNF não é. Apesar dessas formas serem conversíveis uma a outra, a conversão é NP. Esse caso demonstra a importância da representação dos conceitos ao evidenciar que conceitos equivalentes apresentam aprendizados com complexidades diferentes. 

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{Referências} % Change the default bibliography title

\bibliography{references} % Input your bibliography file

%----------------------------------------------------------------------------------------

\end{document}
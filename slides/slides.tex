\documentclass[10pt, professionalfonts]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage[brazil]{babel} % Use brazilian portuguese by default
\usepackage{amsmath}
\usepackage{graphicx, wrapfig}
\graphicspath{ {./imgs/} }
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}

\definecolor{myBeige}{HTML}{faf7f0}
\definecolor{myRed}{HTML}{e45d53}
\setbeamercolor{frametitle}{bg=myBeige, fg=normal text.fg}
\setbeamercolor{background canvas}{bg=myBeige, fg=normal text.fg}

% Configure title graphic positioning
\setbeamertemplate{title graphic}{%
  \vbox to 0pt {
    \vspace*{.75\textheight}  % change the value as necessary
    \hfill\inserttitlegraphic%
  }%
  \nointerlineskip%
}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{xspace}
















\title{Teoria de Aprendizagem Computacional}
\subtitle{Uma Brevíssima Introdução}
\date{\today}
\author{Fred Guth}
\institute{%
Departamento de Ciência da Computação\\%
Universidade de Brasília\\%
\par%
316415\\%
Seminários}

\graphicspath{{./imgs/}}

\titlegraphic{\includegraphics[width=2.5cm]{logoUnB.png}}


\begin{document}

\maketitle





{
\AtBeginSection{}
\section{Preâmbulo}
\begin{frame}{O que é apreensível?}

\begin{columns}[c]
  \column{.6\textwidth}
\textbf{O problema da indução:}\\
Não há justificativa para generalizações baseadas em indução~\cite{Hume2009Tratado}: 
\begin{quotation}
  ``Sempre que no passado comi pão, ele me alimentou. Portanto, da próxima vez que comer pão, ele me alimentará''.
\end{quotation}~\cite{hume2004investigacoes}
  \column{.3\textwidth}
  \centering 
  \begin{figure}
    \label{hume}
    \framebox{\includegraphics[width=1.5in]{hume}}
    \caption{David Hume (1711-1776).}
  \end{figure} 
\end{columns}
\end{frame}

  \begin{frame}{Teoria da Aprendizagem Computacional}

      \centering
      \centerline{\includegraphics<1>[width=1\textwidth]{venn_lt}}
      \includegraphics<2>[width=.4\textwidth]{venn_lt}

      \pause
      \only<2>{
        \begin{columns}[c]
          \column{.3\textwidth}
            \begin{figure}
              \label{valiant}
              \framebox{\includegraphics[width=1.3in]{leslie}}
              \caption{Leslie Valiant, Prêmio Turing 2010.}
            \end{figure}
          \column{.7\textwidth}
          \begin{quotation}
            ``Quantas amostras são necessárias para um algoritmo aprender uma tarefa?''
          \end{quotation}
          \hspace*{1cm}\url{learningtheory.org/colt19}~\cite{COLT}
        \end{columns}
      }
      
  \end{frame}
  \begin{frame}{Modelo de Aprendizado PAC: Definição Informal}
    \begin{equation}
      \underbrace{\text{ Provavelmente }}_{\textrm{confiança} >(1-\delta)}
      \underbrace{\text{ Aproximadamente }}_{\textrm{tolerância} \leq\epsilon}
      \underbrace{\text{   Correto   }}_{\textrm{Erro Absoluto} = 0} \nonumber
    \end{equation}
    \pause
  \begin{itemize}
    \item Com alta probabilidade, o erro é pequeno.
    \item Amostras de uma distribuição arbitrária, mas a mesma para treinamento e teste.
    \item Complexidade computacional e de amostras limitadas polinomialmente ao inverso dos erros.
  \end{itemize}
  \end{frame}
}




{
\AtBeginSection{}
\section{Modelo de Aprendizado PAC: Definição formal}
\begin{frame}{Visão Geral}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}
\begin{frame}{Modelo de Aprendizado PAC: Notações e Definições}
  \begin{description}\setlength\itemsep{1em}
    \item [$\mathcal{X}$]: espaço das instâncias;
    \item [$\mathcal{D}$]: distribuição de probabilidade em $\mathcal{X}$;
    \item [$\mathcal{Y}=\{0,1\}$]: rótulos $\therefore$ problema de classificação;
    \item [$c:\mathcal{X}\to\mathcal{Y}$]: conceito $c \in \mathcal{C}$;
    \item [$\mathcal{A}$]: algoritmo aprendiz;
    \item [$S=\{(x_i, c(x_i))\}$]: conjunto de amostras de treinamento (aprendizado supervisionado);
    \item [$h \in \mathcal{H}$]: hipótese do espaço de Hipóteses;
    \item [$erro_{S}(h)$]: erro de treinamento de $h$, $erro_{S}(h) \equiv \underset{x \sim \mathcal{S}}{P}[c(x) \neq h(x)]$;
    \item [$erro_{\mathcal{D}}(h)$]: erro absoluto de $h$, $erro_{\mathcal{D}}(h) \equiv \underset{x \sim \mathcal{D}}{P}[c(x) \neq h(x)]$;
    
  \end{description}
\end{frame}

% \begin{frame}{Definições: conceito}
%   \begin{figure}[!htp]
%     \centering
%     \includegraphics[width=.8\textwidth]{concept}
%     \label{concept}
%     \caption{Um conceito $c$.}
%   \end{figure}
% \end{frame}
% \begin{frame}{Definições: hipótese}
%   \begin{figure}[!htp]
%     \centering
%     \includegraphics[width=.8\textwidth]{hypothesis}
%     \label{hypothesis}
%     \caption{Uma hipótese $h \in \mathcal{H}$.}
%   \end{figure}
% \end{frame}
\begin{frame}{Definições: aprendiz}
  \begin{figure}[!htp]
  \centering
  \begin{tikzpicture}
      \node[draw, rectangle, line width=0.3mm, minimum width = 3 cm, minimum height = 2 cm] (fl) at (0,0) {$\mathcal{H}$};
      \node[above] at (fl.north) {$\mathcal{A}$};
      \draw[<-] (fl) -- node[above]{$S = \{x_i, c(x_i)\}$} node[below]{} ++(-5,0);
      \draw[->] (fl) -- node[above]{$h$} node[below]{} ++(5,0);
  \end{tikzpicture}
  \caption{Um aprendiz genérico $\mathcal{A}$.}
  \label{fig:diagram}
\end{figure}

\end{frame}
\begin{frame}{Definições: espaço de hipóteses  $\mathcal{H}$}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.95\textwidth]{hspaces}
    \label{hspaces}
    \caption{hipóteses pertencentes a diferentes espaços de hipóteses $\mathcal{H}$.}
\end{figure}

\end{frame}

\begin{frame}{Definições: 2 tipos de erro de $h$}
  \begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{conceptVShypothesis}
    \label{conceptVShypothesis}
\end{figure}
\textbf{Erro de Treinamento:}\begin{equation}
  erro_{S}(h) \equiv \underset{x \sim \mathcal{S}}{P}[c(x) \neq h(x)]
\end{equation}
\textbf{Erro de Absoluto:}
\begin{equation}
  erro_{\mathcal{D}}(h) \equiv \underset{x \sim \mathcal{D}}{P}[c(x) \neq h(x)]
\end{equation}
\end{frame}




}
\begin{frame}{Modelo de Aprendizado PAC: Definição Formal}
  \noindent\fbox{%
    \parbox{\textwidth}{%
    
        $C$ é \textbf{PAC-apreensível} por $\mathcal{A}$  se e somente se, com probabilidade $1 - \delta$, $\mathcal{A}$  gera uma hipótese $h \in \mathcal{H}$ com $erro_{\mathcal{D}}(h)\leq\epsilon$ com número de amostras polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $n=$ tamanho da instância e $tamanho(c)=$ tamanho do conceito, para $0 < \epsilon \leq \frac{1}{2}$ e $0 < \delta \leq \frac{1}{2}$.
    }%
  }

    \bigskip
    \noindent\fbox{%
    \parbox{\textwidth}{%
        $C$ é \textbf{eficientemente PAC-apreensível} por $\mathcal{A}$ se e somente se $\mathcal{A}$ é \textbf{PAC-apreensível} e gera hipótese em tempo polinomial em função de $\rfrac{1}{\delta}$, $\rfrac{1}{\epsilon}$, $n$ e $tamanho(c)$~\cite{MitchelPAC}.
    }%
  }
  
  
\end{frame}
{

  \AtBeginSection{}
  \section{Teorema de Haussler: Limites para $\mathcal{H}$ finito.}
  \begin{frame}{Haussler (1988): Limites para $\mathcal{H}$ finito.}
    \noindent\fbox{%
      \parbox{\textwidth}{%
      \textbf{Teorema: } Seja $\mathcal{H}$ finito, $C \subset \mathcal{H}$ e $\mathcal{A}$ um aprendiz consistente. Se $|S|=m, m \geq 1$, então 
        $P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}$\cite{Haussler1988}
      }%
    }
    \pause
    \begin{proof}  Sejam $h_i (i = 1, ..., k) \in \mathcal{H}$ tais que $erro_{\mathcal{D}}(h_i)>\epsilon$
      \begin{align}
      P_{x_j\sim\mathcal{S}}[(c(x_j) \neq h_i(x_j))=0] \leq (1-\epsilon) \label{haussler:a} \\
      P[\forall h \in |\mathcal{H}|: (erro_{S}(h)=0 \land erro_{\mathcal{D}}(h)> \epsilon)] \leq (1-\epsilon)^m \label{haussler:b} \\
      P[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq k (1-\epsilon)^m \label{haussler:c} \\
      P[\exists h \in H: erro_{\mathcal{D}}(h)>\epsilon]\leq |\mathcal{H}| (1-\epsilon)^m \label{haussler:d} \\ 
      (1-x) \leq e^{-x}, 0 \leq x \leq 1 \implies \nonumber \\ P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}  \label{haussler:e} \\
      \qedhere
      \end{align}
      \end{proof}
      

  \end{frame}
\begin{frame}{Haussler (1988): Implicações}
  \noindent\fbox{%
    \parbox{\textwidth}{%
    $|H|e^{-\epsilon m}=\delta \implies $ aprendiz $\mathcal{A}$ aprende $C$ em $m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|}+\ln{\frac{1}{\delta}})$ amostras de treinamento. 
    }%
}
\pause
\begin{proof}  Dado $P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq |H|e^{-\epsilon m}$ (Haussler) e $P[\exists h \in \mathcal{H}: error_{\mathcal{D}}(h)>\epsilon]\leq \delta$ (PAC), podemos supor:
\begin{align}
    |\mathcal{H}|e^{-\epsilon m}=\delta \implies e^{-\epsilon m} = \frac{\delta}{|\mathcal{H}|} \\
    - \epsilon m = (\ln{\delta} - \ln{|\mathcal{H}|})  \\
    \epsilon m = (\ln{|\mathcal{H}|} - \ln{\delta}) \\
    m = \frac{1}{\epsilon}(\ln{|\mathcal{H}|} + \ln{\frac{1}{\delta}}) \\
    \qedhere
\end{align}
\end{proof}

\end{frame}
}

{

  \AtBeginSection{}
\section{Exemplos de aplicação do modelo PAC}
\begin{frame}{Exemplos de aplicação do modelo PAC}
  \begin{itemize}[<+->]
    \item "Conceito Universal" é PAC-apreensível?
    \item Conjunção de n booleanos é PAC-apreensível?
    \item k-CNF é PAC apreensível?
    \item k-DNF é PAC apreensível?
    \end{itemize}
\end{frame}
}
{

  \AtBeginSection{}
\section{Conclusão}
\begin{frame}{Conclusão}
  \begin{itemize}[<+->]
    \item Aprendizagem Computacional = Teoria da Computação $\cap$  Aprendizagem de Máquina
    \item Podemos limitar \textbf{erro absoluto} mesmo só conhecendo o \textbf{erro de treinamento}
    \item Para determinados $\delta$ e $\epsilon$, o tamanho do conjunto de treinamento necessário \textbf{independe} de $\mathcal{C}$ ou  de $P(\mathcal{X})$.
    \item ... e depende apenas logaritmicamente de $|\mathcal{H}|$
    \item Na prática esses limites são muito "frouxos" \cite{MitchelPAC}: preferência por modelos pequenos não se justifica experimentalmente
    \item  k-DNF vs k-CNF evidencia a importância das representações em aprendizado de máquina
    \end{itemize}
\end{frame}
}
{

  \AtBeginSection{}
\section{Referências}
\begin{frame}[allowframebreaks]{Referências}

  \bibliography{references}
  \bibliographystyle{abbrv}

\end{frame}
}
\begin{frame}<beamer>[standout]
  Obrigado.
  \vspace*{.5cm}
\begin{center}\href{https://github.com/fredguth/learning_theory.git}{\url{github.com/fredguth/learning_theory}}\end{center}
  \vspace*{.5cm}
  \small{
    \begin{center}\ccbysa\end{center}
    \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons \\
      Attribution-ShareAlike 4.0 International License}.
  }
\end{frame}

\appendix

\begin{frame}[fragile]{Para saber mais}
  \begin{itemize}
    \item \href{http://bit.ly/tom_mitchel_learning_theory}{\url{http://bit.ly/tom_mitchel_learning_theory}}
    \item \href{https://www.youtube.com/watch?v=dYMCwxgl3vk&t=1594s}{Hypothesis Space and Inductive Bias: \url{https://www.youtube.com/watch?v=dYMCwxgl3vk&t=1594s}}
  \end{itemize}
\end{frame}



\end{document}
